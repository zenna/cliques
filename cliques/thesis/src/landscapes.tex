\chapter{Combinatorial Energy Landscapes}
\label{chap:landscapes}

The notion of a landscape has emerged as one of the unifying themes in the study of complex systems \cite{Reidys}; and bolstered by our increasing computational ability to explore such surfaces, has proved key in understanding the underlying properties of systems from many domains.  A combinatorial landscape is based upon three fundamental requirements \cite{Reidys}: the first is set of configurations, or discrete conformations which objects in this space can realise.  Second is a function $ f:V\rightarrow  $ $ \mathbb{R} $ which assigns to or represents a quantity of a given configuration.  Lastly is the notion of neighbourhood or locality between configurations.  In the molecular sciences, it is the \textit{potential energy landscape} which governs the structure and dynamics of atomic and molecular clusters, and is defined as a function of all the relevant atomic or molecular coordinates.  Conceptual parallelisms have been made between the PES and biological fitness landscapes at the heart of the dynamics of evolutionary optimisation theory \cite{Reidys}.  It is these domains, from which we extend the notion of landscapes to all connected partitions of a graph, and from which we draw a plethora of analytical tools.

\section{Defining The Complete Partition Landscape}
\label{sec:definingTheLandscape}

Having uncovered all connected partitions to satisfy the first requirement of a combinatorial landscape as defined previously, we bring our attention to the second; namely the function evaluating objects in our configuration space, or specifically partitions of a graph.  We apply a quality function, which aims to uncover its community structure, giving preference to partitions which highlight clusters or modules which are more densely intra-connected than inter-connected.  A quality function assigns a number, where partitions with high scores are good and the highest scoring partition is deemed the best.  Yet, as long as the concept of community evades a rigorous definition, the question of goodness or quality of a partition is ill-posed \cite{Fortunato}.  Indeed the discrepancies between an intuitively good partition as deemed by a human observer, and that declared as optimal by commonly used quality functions, forms much of the basis of their criticism in recent research \cite{Good2010}.  Nevertheless quality functions have achieved success in multiple practical applications, the most popular of which being the Newman-Girvan modularity\cite{Newman2004}. 
\[ 
Q = \frac{1}{2m}\sum_{ij}(A_{ij}- P_{ij})\delta C_iC_j
\]
The modularity compares the edge density within a community to that expected in an equivalent null model. It is based on the notion that the null model should have no community structure and thus the modularity rewards deviations from this.  In this contribution however we explore the \textit{stability} of a partition \cite{Lambiotte2009}\cite{Delvenne2010a}, explained further in section \ref{sec:stabilityLandscape}, which rewards partitions possessing groups from which a random walker on a graph is unlikely to leave within a given time scale $t$.

\subsection*{Locality}
To define locality we must establish a set of modifications to a partition which determine its neighbours within the configuration space.  Unlike on potential energy surfaces, where locality can be defined with physical meaningfulness as single changes in atomic coordinates; in the partition landscape no single definitive moveset exists.  Here we implement the single node moveset \cite{Good2010}, which transplants a node from one group into another or into its own new singleton group.  We use this approach both when finding all neighbours of a partition for small graphs and when finding a random neighbour for use in sampling methods for the tackling of larger graphs.  Under this moveset the total number of neighbours of a partition is bounded by $n_1p-n_2(p-1)$ where $n_1$ is the number of nodes belonging to a non singleton set, $n_2$ is the number that do belong to their own single community, and $p$ is the number of parts, or groups of the partition.  In practice this value is greatly reduced when the constraint of partition connectedness is met.

Although permissible in graph theory, a \textit{disconnected} partition, i.e. one which contains groups where not all nodes are mutually accessible within the group, has questionable relevance in relation to community structure.  Although neither the modularity nor stability explicitly penalise disconnectivity of a partition, disconnectivity effectively decreases intraconnectivty of a module and the result would be the same.  If two nodes are accessible only through neighbours belonging to different groups, then it could be argued that they can not belong to the same community with any meaningfulness.  In this contribution we consider only connected partitions of a graph and consider this the complete enumeration.

\section{Feautures of the Landscape}
\label{sec:featuresOfTheLandscape}

A potential energy surface is usually characterised by its stationary points or local optima: nuclear configurations where all forces vanish and every component of the gradient vector is zero\cite{Wales2005}.  The global minimum corresponds to the most stable nuclear conformation, and thermodynamical processes relax to local minima.  Inversely in the partitions landscape, it is often maxima which are more of interest, as they typically represent the best community structure of a network.  That said, it is entirely possible that other functions assessing partitions from a perspective entirely removed from community, could be used to reveal interesting properties of the system.

\subsection*{Finding Stationary Points}
With access to the complete landscape finding stationary points is straightforward.  For each partition we follow a path of steepest ascent to its highest quality neighbour, and continue to do so through sucessive partitions until a local maxima is found.  The only complication is the case of neutrality, whereby neighbouring maxima possess identical quality scores.  In this case we label all maximal neighbours as maxima; but note with caution that for landscapes encompassing large flat planes, it may be more useful to label none of these as maxima.

\subsection*{Bottlenecks: Transitionary Stationary Points}
In \cite{Wales2005} Wales argues that is insufficient to simply characterise a large number of minima, and that their connectivity on the potential energy surface is critical in determining the relaxation dynamics.  He goes on to explain transition states, local maxima on pathways between local minima; or alternatively defined as dividing high dimensional surfaces that represent dynamical bottlenecks.  Although no thermodynamic relaxation processes are occurring on the partition landscape, useful information about the underlying system is present in the connectivity of local maxima.

In order to define these transition states for the partition landscape, we look for the path between two maxima which reduces least in quality.  To find this we first consider our landscape as a graph, with nodes representing partitions and the edges connecting single node moveset neighbours.  The path between two maxima can then be attacked with the diverse array of graphical search algorithms, for which there is extensive literature and implementations available.  One possibility would be to modify the widely used Dijkstra algorithm to make the priority queue return the node with the highest energy value instead of the closest in the conventional sense.  However as noted in \cite{Berlin2006} the question of whether or not there exists a path of at least capacity $k$, can trivially be solved by removing all edges less than $k$.  It then seems reasonable to suggest that this problem is simpler than the shortest path problem and should be solvable in better time than Dijkstra algorithm $O(m+n\log n)$.  In fact by formulating the partition landscape as a flow network with edges acquiring their capacity from the quality of connecting nodes, the transition states can be found in linear time.

The goal of the Bottleneck Shortest Path Problem is to determine the limiting capacity of any path between two specified vertices of the network. Here we formally reproduce the problem as defined in \cite{Berlin2006}: let $G = (V,E)$ be a graph, with edge weights $c_e \in \mathbb{N}$ for all edges $e \in E$. The \textit{capacity} $b_p$ of a \textit{path} $p$ is given by $b_p := \min_{e \in p} c_e$.  With respect to some fixed start vertex $s \in V$, the bottleneck $b_t$ of a vertex $t \in V$ is $b_t := max_p b_p$, where $p$ ranges over all from $s$ to $t$. The defining edge in the capacity of the path is labelled \textit{critical} for the path or vertex. The Bottleneck Shortest Path Problem (BSP) is to find this critical edge, or bottleneck $b_t$ of some specified target vertex $t$.

The algorithm works by first finding the median edge weight, and removing edges from the graph $G$ below that weight.  This process is iterated until $s$ and $t$ are disconnected, i.e. they belong to separate connected components.  Once this requirement is met, it is clear that the critical edge must be one of the edges removed in the most recent iteration.  A new graph $G'$ is created with a single node for each of the connected components of the now $s-t$ disconnected graph $G$.  For every removed edge, if that edge connected two separate components of $G$, an edge is drawn between nodes representing those components in the $G'$.  If several of the previously removed edges connect the same nodes components of $G$, the representative edge in $G'$ takes the weight of the highest.  This entire algorithm is recursed using the new graph $G$ as the input, and  is repeated until the single critical edge remains.

\begin{figure}[h]
  \centering
  \includegraphics[width=14cm]{bspseqnew.pdf}
  \caption{Each step in the BSP algorithm: from ten edges to the critical edge}
\end{figure}

By assigning weights to all edges of our graph as the quality score of its lesser connected node, the bottleneck shortest path algorithm will uncover the transition state between a source and target local maxima.  We can appreciate this as reforming the landscape as a series of pipes, whose capacities decrease as you descend the landscape.  The bottleneck shortest path uncovers from all possible paths connecting the source and target, the capacity of the thinnest pipe in the path which decreases least in flow.  Uncovering the actual path can be done by simply removing all edges of capacity below the found bottleneck and using simple linear time search methods.
\clearpage
\section{Disconnectivity Graphs}
\label{sec:dgGraphs}

To analyse the landscape we inherit the disconnectivity graph approach of Becker and Karplus in \cite{OMBecker1997}, and used extensively by Wales in \cite{Wales2005}.  For a potential energy surface, given a database of local minima and transition states, a disconnectivity graph can be constructed as follows. For total energy, $E$, the local minima can be divided into disjoint sets, or \textit{superbasins}. The members of each superbasin can be interconverted by a single or more conformational rearrangements without exceeding a defined threshold energy; while for members of different sets, the minimum energy required to interconvert these exceeds \textit{E}. This analysis to form superbasins is done over a set of discrete energy levels $E_1 < E_2 < E_3$ ..., and each superbasin is represented by a node on the horizontal axis. Edges are drawn upwards starting from the energy of each local minimum, to the node corresponding to the its parent node at a higher energy.

With the transition states found through the bottleneck shortest path method, we have all necessary components to create the disconnectivity graph of the partition landscape.  One small modification is to start with maximal quality and reduce in in steps $Q_1 > Q_2 > Q_3 >$ ...    As a result our disconnectiivty graphs as shown in figure \ref{fig:randomvsnormaldg} and \ref{fig:differentdgs} are inverted with respect to that of a PES.

As shown in consequent sections, the disconnectivity graph is not the only method of visualising a landscape and indeed other projections may provide a richer representation from certain perspectives.  However the disconnectivity graph characterises many import aspects of the landscape, and as a graph, lends itself nicely to quantitative analysis.  We have presented here the disconnectivity graph found using the complete partition landscape, with all maxima being found with certainty and all transition states being determined to exactness.  We envision however that this approach could be extended to much larger graphs; with sampling techniques used to uncover local maxima, optimisation strategies to find transition states and potentially the appliance of several more techniques from theoretical chemistry to to its analysis.

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{randomvs12.pdf}
  \caption{a) The disconnectivity graph of a 12 node graph (a)random and modular (b) graph.  The random graph exhibits no branching which was consistent among all Markov times.  The modular graph however, reveals two levels of branching.}\label{fig:randomvsnormaldg}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{dgsatminima.pdf}
  \caption{A comparison of disconnectivity graphs at different Markov times.  Figures in (a) and (b) are taken from points surrounding local minima in the variation of information, labelled in (c), showing the change in the DG when a robust partition is found.  At the the first local minima a new branch emerges in the DG (a.2) and it returns to its original shape at (a.3).  The second minima also increases in the levels of branching, but the DG remains the same in (a.3) (note: apparent differences in (a.2) and (a.3) are artefacts of the drawing procedure and both are equivalent).  Also interesting in (a) is the relatively long separation between regions of branching structures, indicating a separation of maxima into different groups.  Specifically in this case there seems to be a number of higher stability maxima which can easily reach each other without much loss in stability, and and a second set of lower stability with the same interconnectivity.}\label{fig:differentdgs}
\end{figure}


\clearpage
\afterpage{\clearpage}

\section{Stability as dynamic energy landscape}
\label{sec:stabilityLandscape}

The modularity, although very successful has several limitations and many extensions or completely new quality functions are often introduced.  Here we briefly outline the stability of a network partition, a measure of its quality defined in terms of the properties of a dynamical process occurring a graph. Stability takes a further parameter, the markov time; which allows it to uncover uncovers community structures at different resolutions and describes a Markov process in terms of the motion of a random walker moving on the graph.  It is defined in \cite{Lambiotte2009} for a partition $\mathcal{P}$ as
\[
R_{\mathcal{M}}(t) = \sum_{C \in \mathcal{P}} P(C,t) − P(C, \infty )
\]
where $P(C, t)$ is the probability of a walker being in the same community $C$ initially and at times $t$.
$R_{mathcal{M}}(t)$ can thus be used as a fitness function to be maximised, uncovering optimal partitions over an entire time span.  For insight into stability we refer to \cite{Lambiotte2009}\cite{Delvenne2010a}, but mention that we use the normalised version of the stability for computations unless otherwise stated.
\[ 
R_{NL}(t) = \sum_C\sum_{i,j C}\bigg[\Big( e^{t(B-I)} \Big)_{ij}  \frac{k_j}{2m}-\frac{k_i}{2m}\frac{k_j}{2m}\bigg]
\]
Where $B_{ij} = A_{ij}/k_j$ and $I$ is the identity matrix.
At $t = 0$ $R_{NL}(0)$ is maximimal when each node is within its own singleton community. On the other hand, as $t \rightarrow \infty$, $R_{NL}(t)$ is usually maximised by a partition of two groups.  Therefore As time grows, hierarchical organisation of the underlying system can be revealed, which would otherwise not be possible when only looking for a single partition.

In this sense, our landscape becomes a a dynamical surface which changes in morphology through time, more akin to an ocean than rigid surface.  This extra dimension, time, should provide more information about the dynamics of the underlying system; but in an already highly dimensional combinatorial space, it brings with it further difficulty in exploration.  New techniques for uncovering the significant and interesting features of the dynamical landscape become a necessity.

\subsection*{Finding Optimal Stability Partitions}
With a time dependent quality measure, we can apply optimisation techniques to uncover the optimally stable partition at a particular time.  The possible strategies are vast, with the potential to draw from methods in data science, which refers to communities as clusters in data and typically operates based on some notion of similarity between partitions.  Similarly there are several techniques designed specifically for graphs, where modularity optimisation techniques would could be implementation for a given $t$ with no modification. In this work we use here the Louvain algorithm.  It is an agglomerative model, which commences by placing each node into its own and group and then to group atoms, first one by one, then by groups, into larger communities, with each grouping of a node only considering if it increases the overall stability.  The algorithm ceases when no increase in stability can be obtained. As typical in agglomerative methods, the resulting clustering can depend on the initial conditions, i.e. which nodes are moved into another group first.  This stochastic element, while seen as undesirable for some purposes, has been used in \cite{Antionne} as an effective means of characterising the \textit{delmotte} of a partition. 

\subsection*{Robust Partitions}
Given our understanding of stability we realise it is likely that for certain times, there may be no single good partition which contains random walkers within its modules to a high probability, despite being optimal in stability sense.  In other words as put succinctly in \cite{Delmotte} although a partition which identifies a strong community structure has a high stability, not every partition with a high stability corresponds to a strong community structure.  It is therefore perhaps more useful to look for robust partitions; those optimal in stability over significant time scales and resilient to changes in initial conditions.  The efficiency of the Louvain method allows us optimise a large number of times for every time step; and given different conditions we can observe how vulnerable the optimal partitions are.  The variation of information, explained further \ref{subsec:visland}, allows us to quantify the variation in the optimal partitions given different initial conditions and serves as a proxy for robustness.  Because the number of possible partitions is very different at different times scale, for instance at small times scales only singleton partitions are uncovered; the variation of information can not be used an an absolute robustness measure across all $t$.  In practice this leads to qualitatively observing the measure over time in comparison to expected or previously experiences values, and with the prior understanding of its relevance or irrelevance at certain times.  

Under the hypothesis that the existence of good and robust partition at a given time necessitates a change in the stability of other partitions at that time, it seems clear that this must be reflected in the dynamics of the overall landscape.  In other words, by observing some metrics of the all partitions over time it should be possible to pick out those partitions which are robust.  Before metrics used to quantify properties of the landscape can be defined, it seems sensible to generate some intuition as to what change might occur in response to the emergence of good partitions.  Visualising the landscape first, is a good means to achieve this.

\subsection*{Visualising the landscape}
\label{subsec:visland}
In \cite{Good2010} Good \textit{et al.} visualised the partition landscape to investigate degeneracy of modularity such that the globally maximum modularity partition is typically hidden among an exponential number of structurally dissimilar, high-modularity solutions.  We use their method to visualise the landscape in three dimensions to qualitatively analyse the stability landscape at different times.  Whereas Good implemented simulated annealing to sample portions of the landscape and favoured a projection resulting in smoothness, we crucially have the complete enumeration of partitions to project.

To reconstruct and visualise partitions, they are projected as points in a two dimensional Euclidean space while preserving the pairwise distances between partitions.  The distance metric used is the variation of information and defined for partitions $\mathcal{P}$ and $\mathcal{P'}$ as:
\[ 
VI(\mathcal{P},\mathcal{P'})=H(\mathcal{P},\mathcal{P'})-I(\mathcal{P},\mathcal{P'})
 \]
where $H$ is the joint entropy and $I$ is the mutual information between the two partitions.  The projection part of the process, tries to convert this matrix of pairwise correlations to Euclidean coordinates such that they can be visualsed.  This is done using curvilinear component analysis, which is closely related to multidimensionality scaling.  It essentially minimises a cost function, where cost denotes an error between the pairwise distances in the original space and the Euclidean distance in the projected space.  In curvilinear analysis, local accuracy is favoured to global accuracy. As shown in figure \ref{fig:3d} only the relative positions are meaningful, and height on the landscape is determined by its stability at a particular time.  We favour local accuracy over global accuracy because it gives a better interpretation of the smoothness or ruggedness of the combinatorial space, which as discussed later is the basis of our analysis.  Another important note is that projection into a Eucilidian two dimensional space is still insufficient to visualise a three dimensional \textit{surface}.  This is because the faces of this surface must have three distinct vertices, and coordinates do not explicitly provide this.  Here, a uniform grid is fitted to the surface, with the height of each point on this grid interpolated from surrounding non-uniformly spaced data points.  As shown, varying $t$ allows us to visualise the landscape change as a parameter of time.

\begin{figure}[h]
  \centering
  \includegraphics{threedees3.pdf}
  \caption[The evolution of the stability landscape over time]%
  {The stability landscape projected into Euclidean space for $t=1.0$
  and its evolution over time $t=0.001, 0.5,1.0,10$}\label{fig:3d}
\end{figure}

The most obvious observation is that the landscape reduces in stability over time.  More interesting is that its topology changes from being relatively flat when $t$ is small and a stable partition is present, to becoming very convex or bowl shaped for large $t$.  It appears that the landscape becomes more complex in the absence of a good stable partition, and with more variance in its depth.  A further interesting point is that the maxima tend to be located around the peripheral of the space, and the presence of a low stability tail region.  As shown in figure \ref{fig:landscapetimes}, the temporal dynamics of the landscape varies greatly for different graphs.

\begin{figure}[h]
  \centering
  \includegraphics{landscapetimesrast.pdf}
  \caption[The evolution of the stability landscape over time]%
  {The stability landscape for different graphs over times $t=10^-4, t=1, t=10^4$.  In the modular graph, the optimal partition is visible throughout the time span (seen as the hot or cold spot the in lower left perimeter of graph).  Interesting at small times this region has the a lower stability than its surrounding area, despite being optima for the majority of $t$.   As expected, the random graph shows no clear persistent optima through time, although large scale regions remain relatively well defined at different $t$.  Also as expected the complete graph has a high degree of homogeneity, and notably the difference at time $t=1$ and $t=10^4$ is barealy noticable.  The path graph is symmetrical at all $t$, which again is intuitive.  From comparison of the random graph where there is no clear good partiton and the modular graph where there is, we can hypothesise that the presence of good partitons will affect the complexity of the entire landscape}\label{fig:landscapetimes}
\end{figure}

\clearpage
\section{Complexity of the landscape}
\label{sec:landscapeComplexity}

When analysing the complexity of the landscape we come to borrow more from the fields of evolutionary biology and function optimsiation than potential energy surfaces. The idea of a fitness landscape underpins the dynamics of evolutionary optimization, whereby genotypes are neighbours through single mutations.  In many fields to which optimisation is key, the understanding through computer simulations that a complete understanding of the dynamics is impossible without a thorough investigation of the underlying landscape has been formed.

Indeed in modularity or stability maximisation, by far the most popular class of methods to detect communities in graphs, our partition landscape \textit{is} a fitness landscape and finding the globally optimal partition is an NP complete problem.  So while our motivations investigating the landscape are different, it is likely an understanding of the landscape could contribute to the development of algorithms to exlpore them.  This problem of identifying the correct algorithm for the correct problem, is a significant challenge for computational intelligence \cite{Fortunato}, as there appears to be no super optimal algorithm superior to all others.  We suggest the utilisation of these tools, designed to help uncover the structure of a landscape in order to guide better search techniques, can be adapted to the partition landscape of a graph to uncover dynamics of the underlying system.

The number of maxima is probably the simplest measure of complexity of a landscape; typically increasing when the landscape becomes more complex.  However its coarse grainedness may introduce some kind of resolution limit, and it also suffers from being misleading in the presence of flat planes throughout the surface as previously mentioned.  To supplement we implement two metrics for complexity of the landscape, the first relying on a random walk process applicable to graphs of all sizes, and the second from characterising the disconnectivity graph found in the previous section.

\subsection{Complexity from random Random Walk}
We implement a measure of the \textit{ruggedness} of a discrete landscape defined by Vassilev \textit{et al.} \cite{VKVassilevTCFogarty} and extended by  Malan to continuous surfaces in \cite{Malan2009}.  Vassilev \textit{et al.} conjecture that given a sample of fitness values resulting from a random walk on a problem landscape, and using a suitable encoding, an estimate of entropy of this sample could form the basis for an estimate of the ruggedness of the problem landscape.  The basic idea is to obtain a landscape path by performing a random walk on the landscape. This path is represented as an ensemble of three-point objects, where each object is a point on the path together with its neighbours. Each three point object is classified in \cite{Malan2009} as one of the following:
\noindent
\begin{itemize}
\item neutral (a point together with its neighbours has equal fitness values);
\item smooth (the fitness differences between the three points changes in one direction: a slope);
\item rugged (the fitness differences between the three points changes in two directions: a peak, valley or step)
\end{itemize}

Determining which symbol a given three-ppoint object belongs to depends on the margin of error used to evaluate equality of two values in our landscape.  As this margin of error is increased, the number of objects considered neutral will increase.
Assume that a random walk on a landscape generates the time series of fitness values ${f_t}$ This time series is represented as
a string, $S(\varepsilon)= s_1s_2s_3...s_n$ of symbols $s_i \in \{1, 0, 1\}$, obtained by the function:
\[
 s_i = \Psi_{f_t}(i,\varepsilon) = 
  \begin{cases}
   \-1 & \text{if } f_i - f_{i-1} < \varepsilon \\
   1 & \text{if } \vert f_i - f_{i-1} \vert \leq \varepsilon \\
   0 & \text{if } f_i - f_{i-1} > \varepsilon
  \end{cases}
\]

Based on this definition of the string $S(\varepsilon)$, an entropic measure $H(\varepsilon)$ is defined as follows:
\[ 
H(\varepsilon) = - \sum_{p \neq q}P_{[pq]} \log_6 P_{[pq]}
\]
where p and q are elements from the set $\{1, 0, 1\}$, $P_{[pq]}$ is defined as
\[
P_{[pq]} = \frac{n_{[pq]}}{n}
 \]
and $n_{[pq]}$ is the number of sub-blocks $pq$ in the string $S(\varepsilon)$.  This only considers those elements which are rugged, and as the total number of rugged shapes can be found to be 6, the base of the logarithmic function is set as 6.  This function tells us therefore that high values of $H(\varepsilon)$, mean there is a higher variety in the of rugged shapes in that walk.  This itself is an indication of the ruggedness or complexity of the landscape.  

The parameter $\varepsilon$ determines the sensitivity of $\Psi_{f_t}$ to differences in fitness values. By increasing
the value of $\varepsilon$ the landscape becomes more neutral. The value of $\varepsilon$ at which the landscape becomes flat is called the \textit{information stability} and is denoted by $\varepsilon ∗$. The value of $\varepsilon ∗$ is therefore the difference in stability values between any two neighbouring points on the landscape.  
Malan extends this to propose a single value to characterise the ruggedness of a landscape, simply:
\[
R_f = \max{} H({\varepsilon})
\]
We calculate use $R_f$ as a metric for the complexity of our partition landscape.  We perform a random walk one million steps in order to estimate $\varepsilon *$, and then average the results of 30 successive walks, each starting from a random position and consisting of 100,000 steps.

\subsection{Complexity from the Disconnectivty Graph}
In potential energy surfaces the landscape complexity measure tells us at which energy regime branching of the main path becomes significant, making the system more likely to be kinetically trapped in local minima.  Rylance \textit{et al.} use Shannon entropy to calculate a measure of landscape complexity from the disconnectivity graph \cite{Rylance2006}.  They define the residential probability $P_r$ of superbasin using a connectivity index $(n,m)_{V_i}$ with n the index of the parent node of $s$ at energy $V_i+1$ and $m$ the index of $s$ over all child nodes of $n$.
\[ 
P_r [(n,m)_{v_i}]=\frac{v[(n,m)_{V_i}]}{\sum'_{n',m'}v[(n',m')_{V_i}]} 
\]

where v n,m vi means the size of the superbasin $[(n,m)_{v_i}]$ and $\sum'$  sums over all superbasins belonging to energy level Vi.  $P_r$ represents the probability of being located within the superbasin when at the specified energy level.  Thus, the residential when plotted as a function of $V_i$ for the superbasins making up root, indicate the change in size of root’s superbasins in relation to all other superbasins at each energy level $V_i$ 


To quantify the topographical complexity of the disconnectivity graph they introduce a landscape complexity measure $C_L(V_i)$, at energy level $V_i$.  It is defined as the Shannon entropy of the relative sizes of the superbasins (i.e. residual probability) at a chosen level.
\[
C_L(V_i)=-\sum_{n,m}P_r [(n,m)_{V_i}] \log P_r [(n,m)_{V_i}]
\]

\begin{figure}[h]
  \centering
  \subfloat{\label{energyarea} \includegraphics[width=7cm]{energyarea.pdf} }
  \subfloat{\label{dada} \includegraphics[width=7cm]{energy2.pdf} }
  \caption[]%
  {Complexity as a function of maximal stability - stability}
\end{figure}

It produces a complexity of zero if there is a single unique superbasin and maximal complexity when the size of all superbasins are equal at energy $V_i$. The landscape complexity measure can then be integrated over a energy or stability range of the disconnectivity graph DG and normalised by this range, to present the single value of complexity $\bar{C}_L$.  In the stability landscape, this is shown in \ref{energyarea} and \ref{dada}, taken at two different Markov times, as the negative area encapsulated by the curve and the $x$ axis.  The more complex of the two has the larger area, and also noteworthy is the difference in shapes	
\[
\bar{C}_L = \frac{1}{V_{max} - V_{min}} \int_{V_{min}}^{V_{max}} C_L(V)dV,
\]
Where $V_{min}$ and $V_{max}$ are minimum and maximum energies in data set.
\section{A comparison of measures}
\label{sec:results}
Figure \ref{fig:comprandomnormal} compares these complexity measures between a small modular graph and a random comparison.  For the random graph, at no point could any real correlation seen between any of the complexity values.  On the other hand the progression from four to three communities, is reflected in a sharp reduction in $\bar{C}_L$ as well as in the number of maxima and the variation of information.  The random walk however did not seem to change noticeably.  Figure \ref{fig:complexityComparison} compares values for a slightly larger graph and similar results are seen.  

\begin{figure}[h]
  \centering
 \includegraphics{randomvsreal.pdf}
  \caption%
 {A comparison of  complexity measures of a random and 12 node modular graph} \label{fig:comprandomnormal}
\end{figure}

\begin{figure}[h]
  \centering
 \includegraphics{comparecomplexitynewpdf.pdf}
  \caption%
 {A comparison of quality measures of a 14 node modular graph.  This is a multiscale structure with potentially two good partitions, which is reflected in the two areas highlighted in the graph.  Different measures detect the presence of different robust partitions, with the only consistent measure being the variation of information.} \label{fig:complexityComparison}
\end{figure}